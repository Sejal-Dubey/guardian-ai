import pandas as pd
import numpy as np
from faker import Faker
import random
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold
import joblib
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ===================================================================
# STEP 1: LOAD DATASETS 
# ===================================================================
print("--- STEP 1: Loading Datasets ---")
try:
    # Load synthetic training data
    train_df = pd.read_csv(r'C:\Users\Sejal\Downloads\guardian-ai\guardian-ai\backend\email\advanced_synthetic_email_fraud_dataset.csv')
    print(f"Loaded training dataset: {len(train_df)} samples")
    
    # Load realistic test data
    try:
        test_df = pd.read_csv(r'C:\Users\Sejal\Downloads\guardian-ai\guardian-ai\backend\email\test_email_fraud_dataset.csv')
        print(f"Loaded test dataset: {len(test_df)} samples")
    except FileNotFoundError:
        print("WARNING: Test dataset not found. Will only use training data for validation.")
        test_df = None
    
    # For better model performance, let's create more test data from existing data
    if test_df is None or len(test_df) < 50:
        print("\nCreating additional test data by splitting training data...")
        X_temp = train_df.drop('label', axis=1)
        y_temp = train_df['label']
        
        # Split with stratification to maintain class balance
        _, test_df_split = train_test_split(train_df, test_size=0.1, random_state=42, stratify=y_temp)
        test_df = test_df_split.sample(n=min(50, len(test_df_split)), random_state=42)
        print(f"Created test dataset with {len(test_df)} samples")
    
except FileNotFoundError as e:
    print(f"ERROR: {str(e)}")
    print("Please ensure dataset files exist.")
    exit()

# ===================================================================
# STEP 2: INITIALIZE MODELS & DEFINE FEATURE FUNCTIONS
# ===================================================================

print("\n--- STEP 2: Initializing Models ---")
print("INFO: Loading RoBERTa model. This may take a moment...")
tokenizer = AutoTokenizer.from_pretrained('roberta-base')
roberta_model = AutoModel.from_pretrained('roberta-base')
print("INFO: RoBERTa model loaded successfully.")

def get_combined_embedding(subject, body):
    """Get RoBERTa embedding for combined subject and body"""
    combined_text = f"Subject: {subject}\n\n{body}"
    inputs = tokenizer(combined_text, return_tensors="pt", truncation=True, max_length=128, padding='max_length')
    with torch.no_grad():
        outputs = roberta_model(**inputs)
    return outputs.last_hidden_state[0][0].numpy()

def get_header_score(sender_email, domain_match):
    """Calculate a score based on email header analysis"""
    header_score = 0.0
    
    # Domain analysis
    try:
        domain = sender_email.split('@')[1]
        suspicious_keywords = ['-security', '-support', '-alerts', '-verify', 'secure-', 'account', 'login', 'update']
        
        # Check for suspicious patterns
        if any(keyword in domain for keyword in suspicious_keywords):
            header_score += 0.3
            
        # Check for domain similarity to real companies
        legitimate_domains = ['microsoft.com', 'apple.com', 'amazon.com', 'paypal.com', 'chase.com', 'bankofamerica.com']
        if any(legit_domain in domain for legit_domain in legitimate_domains) and domain_match == 0:
            header_score += 0.4  # Using legitimate domain but mismatch
    except IndexError:
        header_score += 0.5  # No @ symbol in email
    
    return min(header_score, 1.0)

# ===================================================================
# STEP 3: FEATURE EXTRACTION FOR TRAINING DATA
# ===================================================================

print("\n--- STEP 3: Creating Style Fingerprint ---")
# Create style fingerprint from legitimate training emails
legit_emails = train_df[train_df['label'] == 0]
legit_embeddings = []
for _, row in legit_emails.head(100).iterrows():  # Use more emails for better fingerprint
    embedding = get_combined_embedding(row['subject'], row['body'])
    legit_embeddings.append(embedding)
style_fingerprint = np.mean(legit_embeddings, axis=0)
print(f"Style fingerprint created from 100 legitimate emails.")

print("\n--- STEP 4: Extracting Features for Training Data ---")
X_train_features = []
y_train_labels = train_df['label'].values

for _, row in tqdm(train_df.iterrows(), total=train_df.shape[0], desc="Training Data Features"):
    # Get combined embedding
    embedding_vector = get_combined_embedding(row['subject'], row['body'])
    
    # Get header score
    header_score = get_header_score(row['sender'], row['domain_match'])
    
    # Calculate authorship score based on style fingerprint
    if style_fingerprint is not None:
        similarity = cosine_similarity(embedding_vector.reshape(1, -1), style_fingerprint.reshape(1, -1))[0][0]
        authorship_score = 1.0 - similarity
    else:
        authorship_score = 0.5
    
    # Get additional dataset features
    additional_features = [
        row['num_links'],
        row['num_dollar_signs'],
        row['num_urgent_terms'],
        row['num_personal_pronouns'],
        row['domain_match'],
        row['suspicious_subject'],
        row['signature_mismatch']
    ]
    
    # Combine all features
    feature_vector = np.concatenate([embedding_vector, [header_score], [authorship_score], additional_features])
    X_train_features.append(feature_vector)

X_train_features = np.array(X_train_features)

# ===================================================================
# STEP 4: FEATURE EXTRACTION FOR TEST DATA (SAME PROCESS)
# ===================================================================

if test_df is not None:
    print("\n--- STEP 5: Extracting Features for Test Data ---")
    X_test_features = []
    y_test_labels = test_df['label'].values

    for _, row in tqdm(test_df.iterrows(), total=test_df.shape[0], desc="Test Data Features"):
        # Get combined embedding
        embedding_vector = get_combined_embedding(row['subject'], row['body'])
        
        # Get header score
        header_score = get_header_score(row['sender'], row['domain_match'])
        
        # Calculate authorship score based on style fingerprint
        if style_fingerprint is not None:
            similarity = cosine_similarity(embedding_vector.reshape(1, -1), style_fingerprint.reshape(1, -1))[0][0]
            authorship_score = 1.0 - similarity
        else:
            authorship_score = 0.5
        
        # Get additional dataset features
        additional_features = [
            row['num_links'],
            row['num_dollar_signs'],
            row['num_urgent_terms'],
            row['num_personal_pronouns'],
            row['domain_match'],
            row['suspicious_subject'],
            row['signature_mismatch']
        ]
        
        # Combine all features
        feature_vector = np.concatenate([embedding_vector, [header_score], [authorship_score], additional_features])
        X_test_features.append(feature_vector)

    X_test_features = np.array(X_test_features)

# ===================================================================
# STEP 5: MODEL TRAINING & EVALUATION
# ===================================================================

print("\n--- STEP 6: Model Training & Evaluation ---")

# Split training data for validation
X_train, X_val, y_train, y_val = train_test_split(
    X_train_features, y_train_labels, test_size=0.2, random_state=42, stratify=y_train_labels
)

print(f"Training set size: {len(X_train)} samples")
print(f"Validation set size: {len(X_val)} samples")
if test_df is not None:
    print(f"Test set size: {len(X_test_features)} samples")

# Train multiple models for better performance
models = {
    "Random Forest": RandomForestClassifier(
        n_estimators=300, 
        random_state=42, 
        n_jobs=-1, 
        max_depth=8, 
        class_weight='balanced',
        min_samples_split=20,
        min_samples_leaf=10
    ),
    "Gradient Boosting": GradientBoostingClassifier(
        n_estimators=150, 
        random_state=42, 
        max_depth=4, 
        learning_rate=0.03,
        min_samples_split=20,
        min_samples_leaf=10
    )
}

best_model = None
best_model_name = ""
best_accuracy = 0
best_threshold = 0.5  # Default threshold
best_precision = 0
best_recall = 0
best_f1 = 0

print("\nTraining and evaluating models...")
for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train, y_train)
    
    # Get probabilities for different threshold evaluation
    val_probs = model.predict_proba(X_val)[:, 1]
    
    # Test different thresholds
    best_threshold_for_model = 0.5
    best_threshold_accuracy = 0
    best_threshold_precision = 0
    best_threshold_recall = 0
    best_threshold_f1 = 0
    
    for threshold in [0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:
        val_predictions_threshold = (val_probs >= threshold).astype(int)
        threshold_accuracy = accuracy_score(y_val, val_predictions_threshold)
        threshold_precision, threshold_recall, threshold_f1, _ = precision_recall_fscore_support(
            y_val, val_predictions_threshold, average='binary', zero_division=0
        )
        
        # We want to balance all metrics
        threshold_score = 0.4 * threshold_accuracy + 0.3 * threshold_precision + 0.3 * threshold_f1
        
        if threshold_score > 0.4 * best_threshold_accuracy + 0.3 * best_threshold_precision + 0.3 * best_threshold_f1:
            best_threshold_accuracy = threshold_accuracy
            best_threshold_precision = threshold_precision
            best_threshold_recall = threshold_recall
            best_threshold_f1 = threshold_f1
            best_threshold_for_model = threshold
    
    # Predict with best threshold
    val_predictions = (val_probs >= best_threshold_for_model).astype(int)
    val_accuracy = accuracy_score(y_val, val_predictions)
    
    print(f"{name} Validation Metrics with threshold {best_threshold_for_model}:")
    print(f"  Accuracy: {val_accuracy:.4f}")
    print(f"  Precision: {best_threshold_precision:.4f}")
    print(f"  Recall: {best_threshold_recall:.4f}")
    print(f"  F1: {best_threshold_f1:.4f}")
    
    # Use a balanced score for model selection
    model_score = 0.4 * val_accuracy + 0.3 * best_threshold_precision + 0.3 * best_threshold_f1
    
    if model_score > best_accuracy:
        best_accuracy = val_accuracy
        best_precision = best_threshold_precision
        best_recall = best_threshold_recall
        best_f1 = best_threshold_f1
        best_model = model
        best_model_name = name
        best_threshold = best_threshold_for_model

print(f"\nBest model: {best_model_name} with balanced score and threshold {best_threshold}")
print(f"Validation Metrics - Accuracy: {best_accuracy:.4f}, Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1: {best_f1:.4f}")

# Perform 5-fold cross-validation on best model
print("\nPerforming 5-fold cross-validation on best model...")
cv_scores = cross_val_score(best_model, X_train_features, y_train_labels, cv=5, scoring='accuracy')
print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Average CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Calculate precision, recall, F1 for CV with the optimal threshold
cv_precision_scores = []
cv_recall_scores = []
cv_f1_scores = []

skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
for train_idx, val_idx in skf.split(X_train_features, y_train_labels):
    X_cv_train, X_cv_val = X_train_features[train_idx], X_train_features[val_idx]
    y_cv_train, y_cv_val = y_train_labels[train_idx], y_train_labels[val_idx]
    
    # Clone the best model and train
    cv_model = type(best_model)(**best_model.get_params())
    cv_model.fit(X_cv_train, y_cv_train)
    
    # Get probabilities and apply threshold
    cv_probs = cv_model.predict_proba(X_cv_val)[:, 1]
    cv_predictions = (cv_probs >= best_threshold).astype(int)
    
    # Calculate metrics
    cv_p, cv_r, cv_f, _ = precision_recall_fscore_support(y_cv_val, cv_predictions, average='binary', zero_division=0)
    cv_precision_scores.append(cv_p)
    cv_recall_scores.append(cv_r)
    cv_f1_scores.append(cv_f)

print(f"Average CV Precision: {np.mean(cv_precision_scores):.4f}")
print(f"Average CV Recall: {np.mean(cv_recall_scores):.4f}")
print(f"Average CV F1 Score: {np.mean(cv_f1_scores):.4f}")

# Check if we should save the model
accuracy_threshold = 0.8  # Maintained at 80%
precision_threshold = 0.65  # Lowered to 65% to be more realistic
recall_threshold = 0.8    # Maintained at 80%

print(f"\nValidation Metrics - Accuracy: {best_accuracy:.4f}, Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1: {best_f1:.4f}")

# Save model if it meets thresholds
if (best_accuracy >= accuracy_threshold and 
    best_precision >= precision_threshold and 
    best_recall >= recall_threshold):
    
    print("\nModel meets all thresholds. Retraining on full dataset...")
    final_model = type(best_model)(**best_model.get_params())
    final_model.fit(X_train_features, y_train_labels)
    
    # Save the model and threshold
    model_info = {
        'model': final_model,
        'threshold': best_threshold,
        'model_type': best_model_name
    }
    
    joblib.dump(model_info, 'fraud_detection_model_advanced.joblib')
    np.save('style_fingerprint_advanced.npy', style_fingerprint)
    print("Model saved as 'fraud_detection_model_advanced.joblib'")
    print(f"Optimal prediction threshold: {best_threshold}")
    print("Style fingerprint saved as 'style_fingerprint_advanced.npy'")
    
    # Evaluate on test data if available
    if test_df is not None:
        print("\nEvaluating on test data...")
        test_probs = final_model.predict_proba(X_test_features)[:, 1]
        test_predictions = (test_probs >= best_threshold).astype(int)
        test_accuracy = accuracy_score(y_test_labels, test_predictions)
        
        print(f"\nTest Accuracy: {test_accuracy:.4f}")
        
        # Print test classification report
        print("\nTest Classification Report:")
        print(classification_report(y_test_labels, test_predictions))
        
        # Print confusion matrix
        print("\nTest Confusion Matrix:")
        print(confusion_matrix(y_test_labels, test_predictions))
    
    print("\n--- âœ… PROCESS COMPLETE - Model Saved ---")
else:
    print(f"\nModel does not meet thresholds.")
    print(f"Required: Accuracy >= {accuracy_threshold}, Precision >= {precision_threshold}, Recall >= {recall_threshold}")
    print(f"Actual: Accuracy={best_accuracy:.4f}, Precision={best_precision:.4f}, Recall={best_recall:.4f}")
    print("Consider adjusting model parameters or getting more training data.")

# Feature importance analysis (for tree-based models)
if hasattr(best_model, 'feature_importances_'):
    feature_importance = best_model.feature_importances_
    print("\nTop 10 Most Important Features:")
    # Get feature names
    n_embedding_features = len(X_train_features[0]) - 9  # All features except the 9 additional ones
    feature_names = []
    feature_names.extend([f'embedding_{i}' for i in range(n_embedding_features)])
    feature_names.extend(['header_score', 'authorship_score'])
    feature_names.extend(['num_links', 'num_dollar_signs', 'num_urgent_terms', 
                         'num_personal_pronouns', 'domain_match', 'suspicious_subject', 
                         'signature_mismatch'])
    
    # Create DataFrame for better readability
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(importance_df.head(10).to_string(index=False))